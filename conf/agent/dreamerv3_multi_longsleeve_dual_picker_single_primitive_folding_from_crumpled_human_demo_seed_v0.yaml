name: dreamerV3
logdir: null
traindir: null
evaldir: null
offline_traindir: ''
offline_evaldir: ''
seed: 0
deterministic_run: False
action_steps: 1e5
parallel: False
# eval_every: 1e4
# eval_episode_num: 10
log_every: 1e2
reset_every: 0
device: 'cuda:0'
compile: True
precision: 32
debug: False
train_video_pred_log: True


# time_limit: 1000
grayscale: False

reward_EMA: True

# Model
dyn_hidden: 512
dyn_deter: 512
dyn_stoch: 32
dyn_discrete: 32
dyn_rec_depth: 1
dyn_mean_act: 'none'
dyn_std_act: 'sigmoid2'
dyn_min_std: 0.1
grad_heads: ['decoder', 'reward', 'cont', 'state']
units: 512
act: 'SiLU'
norm: True
encoder:
  {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
decoder:
  {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
actor:
  {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
critic:
  {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
reward_head:
  {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
cont_head:
  {layers: 2, loss_scale: 1.0, outscale: 1.0}
dyn_scale: 0.5
rep_scale: 0.1
kl_free: 1.0
weight_decay: 0.0
unimix_ratio: 0.01
initial: 'learned'



model_lr: 1e-4
opt_eps: 1e-8
grad_clip: 1000


opt: 'adam'

# Behavior.
discount: 0.997
discount_lambda: 0.95
imag_gradient: 'dynamics'
imag_gradient_mix: 0.0
eval_state_mean: False

# Exploration
expl_behavior: 'greedy'
expl_until: 0
expl_extr_scale: 0.0
expl_intr_scale: 1.0
disag_target: 'stoch'
disag_log: True
disag_models: 10
disag_offset: 1
disag_layers: 4
disag_units: 400
disag_action_cond: False

restart: true
# evaluate_method: 'simulate'
# eval_video_pred_log: true


# Training
batch_size: 128 #16
batch_length: 8 #64
train_ratio: 512

state_head:
  {layers: 2, dist: 'normal', loss_scale: 1.0, outscale: 0.0}
state_dim: 30
imag_horizon: 2 #15
state_key: semkey_norm_pixel
updates_per_step: 5

dataset_size: 100000
total_update_steps: 1e5 #This is action steps actually in dreamer
eval_checkpoint: -1
size: [64, 64]
action_repeat: 1

validation_interval: 1000
obs_keys: ['image', 'is_first', 'is_terminal', 'semkey_norm_pixel']
reward_key: multi_stage_reward
primitive_integration: 'none'
# primitives:
#   - name: norm-pixel-pick-and-fling
#     dim: 4 # [pick_0, pick_1]name: image2state-sac
#   - name: norm-pixel-pick-and-place
#     dim: 8 #params: [pick_0, pick_1, place_0, place_1]
#   - name: no-operation
#     dim: 0
save_success: True
num_actions: 8

use_bc_policy_to_seed: False 
# bc_policy_config: 'bc_policy_config_name' # use agent_arena to bulid, update steps is decided in the config
# bc_prefill: 1000
# bc_data_augmenter: pixel_based_multi_primitive_augmenter_for_diffusion
human_prefill: 200
human_demo_policy: "human-dual-pickers-pick-and-place"

human_demo_dataset_config:
  data_path: "multi_longsleeve_dual_picker_single_primitive_folding_from_flattened_human_demo_100"
  data_dir: "./data/datasets"
  seq_length: 20
  io_mode: "w"
  obs_config:
    mask: 
      shape: [128, 128, 1]
      output_key: 'mask'
    
    depth: 
      shape: [128, 128, 1]
      output_key: 'depth'
    
    rgb: 
      shape: [128, 128, 3]
      output_key: 'rgb'

  act_config:
    default: 
      shape: [8]
      output_key: 'default'

random_prefill: 1000
pretrain: 5000
traj_length: 20

vis_length: 8
vis_post_steps: 3
vis_prior_steps: 5

exp_name: ${exp_name}
project_name: ${project_name}